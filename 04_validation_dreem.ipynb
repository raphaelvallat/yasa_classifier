{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dreem Open Datasets validation\n",
    "\n",
    "https://pubmed.ncbi.nlm.nih.gov/32746326/\n",
    "\n",
    "https://github.com/Dreem-Organization/dreem-learning-open\n",
    "\n",
    "https://github.com/Dreem-Organization/dreem-learning-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pingouin as pg\n",
    "import sklearn.metrics as skm\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.stats as sp_stats\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", font_scale=1.1)\n",
    "\n",
    "# Load predictions file\n",
    "model = \"eeg+eog+emg+demo\"\n",
    "path_dodh = \"output/cv/%s/pred_dreem_dodh.csv\" % model\n",
    "path_dodo = \"output/cv/%s/pred_dreem_dodo.csv\" % model\n",
    "\n",
    "NUM2STR = {0: \"W\", 1: \"N1\", 2: \"N2\", 3: \"N3\", 4: \"R\"}\n",
    "STR2NUM = {\"W\": 0, \"N1\": 1, \"N2\": 2, \"N3\": 3, \"R\": 4}\n",
    "\n",
    "df = pd.concat([\n",
    "    pd.read_csv(path_dodh, index_col=[0, 1, 2]),\n",
    "    pd.read_csv(path_dodo, index_col=[0, 1, 2])\n",
    "])\n",
    "\n",
    "# Map stages\n",
    "labels = ['N1', 'N2', 'N3', 'R', 'W']\n",
    "cols_stage = df.columns.tolist()[:-2]\n",
    "print(cols_stage)\n",
    "\n",
    "for c in cols_stage:\n",
    "    df[c] = df[c].replace(NUM2STR)\n",
    "    assert np.unique(df[c]).tolist() == labels\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Optional: keep specific dataset\n",
    "# df = df[df['dataset'] == 'dodh'].reset_index(drop=True)\n",
    "\n",
    "# Optional: remove subjects for which the hypnogram doesn't match the EEG size\n",
    "df = df[df['pad'] <= 2].reset_index(drop=True)\n",
    "\n",
    "print(df['subj'].nunique(), 'subjects')\n",
    "print(df.shape)\n",
    "df.head().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) To avoid invalid F1, we remove subjects that do not have all sleep stages in \n",
    "# original scoring. If disabled, make sure to use f1_score(zero_division=1).\n",
    "# n_stage_per_subj = df.groupby('subj')['cons'].nunique()\n",
    "# bad_ss = n_stage_per_subj[n_stage_per_subj != 5].index\n",
    "# df = df[~df['subj'].isin(bad_ss)].reset_index(drop=True)\n",
    "# print(df['subj'].nunique(), 'remaining subjects')\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df.shape[0] / 120:.2f} hours of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall (not reported in the paper)\n",
    "# print(\"Acc.:\\t  %.3f\" % skm.accuracy_score(df['cons'], df['yasa']))\n",
    "# print(\"Kappa:\\t  %.3f\" % skm.cohen_kappa_score(df['cons'], df['yasa']))\n",
    "# print(\"MCC:\\t  %.3f\" % skm.matthews_corrcoef(df['cons'], df['yasa']))\n",
    "# print(\"F1-macro: %.3f\" % skm.f1_score(df['cons'], df['yasa'], average='macro'))\n",
    "# print(\"F1-micro: %.3f\" % skm.f1_score(df['cons'], df['yasa'], average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per each night\n",
    "df_scores = []\n",
    "\n",
    "def perc_transition(col):\n",
    "    return (col != col.shift(1)).sum() / col.shape[0]\n",
    "\n",
    "for sub in tqdm(df['subj'].unique(), leave=False):\n",
    "    df_sub = df[df['subj'] == sub]\n",
    "    yt = df_sub['cons']\n",
    "    yp = df_sub['yasa']\n",
    "    n = yt.shape[0]\n",
    "\n",
    "    sub_scores = {\n",
    "        'dataset': df_sub['dataset'].iloc[0],\n",
    "        'scorer': \"yasa\",\n",
    "        \"pad\": df_sub['pad'].iloc[0],\n",
    "        'dur_min': yt.size / 2,\n",
    "        # % Transitions\n",
    "        'perc_trans_true': perc_transition(yt),\n",
    "        'perc_trans_pred': perc_transition(yp),\n",
    "        # Accuracy\n",
    "        'accuracy': skm.accuracy_score(yt, yp),\n",
    "        'kappa': skm.cohen_kappa_score(yt, yp),\n",
    "        'MCC': skm.matthews_corrcoef(yt, yp),\n",
    "        'f1_macro': skm.f1_score(yt, yp, average='macro', zero_division=1),\n",
    "    }\n",
    "\n",
    "    # F1 for each stage\n",
    "    f1 = skm.f1_score(yt, yp, average=None, labels=labels, zero_division=1)\n",
    "    for f, l in zip(f1, labels):\n",
    "        sub_scores['f1_' + l] = f\n",
    "\n",
    "    # Proportion of each stage (NaN = 0)\n",
    "    prop_true = (yt.value_counts() / n).add_prefix('perc_').add_suffix('_true')\n",
    "    prop_pred = (yp.value_counts() / n).add_prefix('perc_').add_suffix('_pred')\n",
    "    sub_scores.update(prop_true.to_dict())\n",
    "    sub_scores.update(prop_pred.to_dict())\n",
    "\n",
    "    # Append to main dataframe\n",
    "    df_scores.append(pd.DataFrame(sub_scores, index=[sub]))\n",
    "\n",
    "df_scores = pd.concat(df_scores)\n",
    "df_scores.index.name = 'subj'\n",
    "df_scores = df_scores.sort_index(axis=1).set_index(\"scorer\", append=True)\n",
    "df_scores.head().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NaN in perc_XX by zero: CAREFUL\n",
    "# df_scores.isna().sum(0)\n",
    "df_scores.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the median\n",
    "df_scores.median().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = list(sns.color_palette(\"Blues\", n_colors=10, as_cmap=False, desat=1))\n",
    "color_pred = cmap[-1]\n",
    "color_ref = \"tab:orange\"\n",
    "cmap_stages = ['#99d7f1', '#009DDC', 'xkcd:twilight blue', 'xkcd:rich purple', 'xkcd:sunflower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1 = df_scores[['f1_N1', 'f1_N2', 'f1_N3', 'f1_R', 'f1_W']].copy()\n",
    "df_f1.columns = df_f1.columns.str.split('_').str.get(1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4.5, 4.5), dpi=100)\n",
    "sns.stripplot(data=df_f1, palette=cmap_stages, ax=ax, alpha=0.5, linewidth=0.75)\n",
    "\n",
    "plt.xlabel(\"Stage\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.ylim(0, 1)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage discrepancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect size of the percentage of transitions\n",
    "def mean_std(x):\n",
    "    x = x * 100\n",
    "    return f\"{x.mean().round(1)} Â± {x.std(). round(1)}\"\n",
    "\n",
    "display(df_scores[['perc_trans_pred', 'perc_trans_true']].agg(mean_std))\n",
    "pg.ttest(df_scores['perc_trans_pred'], df_scores['perc_trans_true'], paired=True).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop_pred = df_scores.filter(like=\"_pred\").iloc[:, :-1].melt(var_name=\"Stage\", value_name=\"Proportion\", ignore_index=False)\n",
    "df_prop_true = df_scores.filter(like=\"_true\").iloc[:, :-1].melt(var_name=\"Stage\", value_name=\"Proportion\", ignore_index=False)\n",
    "\n",
    "df_prop_pred['Stage'] = df_prop_pred['Stage'].str.split('_').str.get(1)\n",
    "df_prop_true['Stage'] = df_prop_true['Stage'].str.split('_').str.get(1)\n",
    "\n",
    "df_prop_pred['Scoring'] = 'Predicted'\n",
    "df_prop_true['Scoring'] = 'Reference'\n",
    "\n",
    "df_prop = pd.concat((df_prop_pred.reset_index(), df_prop_true.reset_index()))\n",
    "df_prop = df_prop.sort_values(by=['subj', 'Stage', 'Scoring']).reset_index(drop=True)\n",
    "\n",
    "df_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the effect size\n",
    "ptest = df_prop.pairwise_ttests(dv=\"Proportion\", within=['Stage', \"Scoring\"], subject=\"subj\", effsize=\"cohen\").iloc[11:, :].round(3)\n",
    "ef = ptest.loc[:, ['Stage', 'cohen']].set_index(\"Stage\").abs()\n",
    "display(ef)\n",
    "\n",
    "# Boxplot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4.5, 4.5), dpi=100)\n",
    "\n",
    "sns.boxplot(y=df_prop['Proportion'] * 100, x=df_prop['Stage'], hue=df_prop['Scoring'],\n",
    "            hue_order=['Reference', 'Predicted'], \n",
    "            palette=[color_ref, color_pred], \n",
    "            saturation=1, width=0.6, fliersize=0, linewidth=1.5, notch=True)\n",
    "\n",
    "plt.ylim(0, 80)\n",
    "plt.yticks([0, 20, 40, 60, 80])\n",
    "plt.legend(frameon=False, loc=\"upper right\")\n",
    "plt.ylabel(\"Proportion of time in bed (%)\");\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices\n",
    "\n",
    "The normalized confusion matrices show the sensitivity (= recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = 100 * skm.confusion_matrix(df['cons'], df['yasa'], labels=labels, normalize=\"true\")\n",
    "cm = pd.DataFrame(cm, index=labels, columns=labels).round(1)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, dpi=100, figsize=(4.5, 4.5))\n",
    "sns.heatmap(cm, annot=True, vmin=0, vmax=100, cmap=\"Blues\", square=True, cbar=False, fmt=\".1f\")\n",
    "plt.ylabel(\"Reference\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Precision \n",
    "# # skm.precision_recall_fscore_support(df['y_true'], df['y_pred'], labels=labels)\n",
    "# cm = 100 * skm.confusion_matrix(df['cons'], df['yasa'], labels=labels, normalize=\"pred\")\n",
    "# cm = pd.DataFrame(cm, index=labels, columns=labels).round(1)\n",
    "\n",
    "# # Plot\n",
    "# fig, ax = plt.subplots(1, 1, dpi=100, figsize=(4.5, 4.5))\n",
    "# sns.heatmap(cm, annot=True, vmin=0, vmax=100, cmap=\"Blues\", square=True, cbar=False, fmt=\".1f\")\n",
    "# plt.ylabel(\"Reference\")\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage transitions\n",
    "\n",
    "Are most of the errors located around transitions between stages?\n",
    "\n",
    "Here, we use PSG to define the transitions between stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = []\n",
    "\n",
    "for sub in tqdm(df['subj'].unique(), leave=False):\n",
    "    df_sub = df[df['subj'] == sub]\n",
    "    yt = df_sub['cons']\n",
    "    yp = df_sub['yasa']\n",
    "\n",
    "    # Identify stable periods, i.e. the 3 epochs before / after are similar (3 minutes window)\n",
    "    first_ep, last_ep = yt.iloc[0], yt.iloc[-1]\n",
    "    stable = np.logical_and.reduce((\n",
    "        yt.shift(1, fill_value=first_ep) == yt,  # = same as previous one\n",
    "        yt.shift(-1, fill_value=last_ep) == yt, # = same as next one\n",
    "        yt.shift(2, fill_value=first_ep) == yt,\n",
    "        yt.shift(-2, fill_value=last_ep) == yt,\n",
    "        yt.shift(3, fill_value=first_ep) == yt,\n",
    "        yt.shift(-3, fill_value=last_ep) == yt,\n",
    "    ))\n",
    "\n",
    "    # Append to main dict\n",
    "    sub_scores = {\n",
    "        'n_stable': len(stable[stable]),\n",
    "        'n_trans': len(stable[~stable]),\n",
    "        'acc_stable': skm.accuracy_score(yt[stable], yp[stable]),\n",
    "        'acc_trans': skm.accuracy_score(yt[~stable], yp[~stable]),\n",
    "        'mcc_stable': skm.matthews_corrcoef(yt[stable], yp[stable]),\n",
    "        'mcc_trans': skm.matthews_corrcoef(yt[~stable], yp[~stable])\n",
    "    }\n",
    "\n",
    "    # Append to main dataframe\n",
    "    df_trans.append(pd.DataFrame(sub_scores, index=[sub]))\n",
    "\n",
    "df_trans = pd.concat(df_trans)\n",
    "df_trans.sort_index(axis=1, inplace=True)\n",
    "df_trans.index.name = 'subj'\n",
    "df_trans.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average and T-test\n",
    "display(df_trans[['acc_stable', 'acc_trans']].apply(mean_std))\n",
    "\n",
    "pg.ttest(df_trans['acc_stable'], df_trans['acc_trans'], paired=False)\n",
    "# pg.ttest(df_trans['mcc_stable'], df_trans['mcc_trans'], paired=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "sns.set(style=\"ticks\", font_scale=1.1)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2.5, 3.5), dpi=100)\n",
    "sns.boxplot(data=df_trans[['acc_stable', 'acc_trans']], width=0.7, fliersize=0,\n",
    "            saturation=1, color=color_pred, ax=ax)\n",
    "\n",
    "plt.ylim(0.5, 1.01)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xticks([0, 1], ['Stable', 'Transition'])\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High vs low confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf = []\n",
    "\n",
    "for sub in tqdm(df['subj'].unique(), leave=False):\n",
    "    df_sub = df[df['subj'] == sub]\n",
    "    yt = df_sub['cons']\n",
    "    yp = df_sub['yasa']\n",
    "    \n",
    "    highconf = df_sub['confidence'] >= 0.8\n",
    "\n",
    "    # Append to main dict\n",
    "    sub_scores = {\n",
    "        'n_highconf': len(highconf[highconf]),\n",
    "        'n_lowconf': len(highconf[~highconf]),\n",
    "        'acc_highconf': skm.accuracy_score(yt[highconf], yp[highconf]),\n",
    "        'acc_lowconf': skm.accuracy_score(yt[~highconf], yp[~highconf]),\n",
    "        'mcc_highconf': skm.matthews_corrcoef(yt[highconf], yp[highconf]),\n",
    "        'mcc_lowconf': skm.matthews_corrcoef(yt[~highconf], yp[~highconf])\n",
    "    }\n",
    "\n",
    "    # Append to main dataframe\n",
    "    df_conf.append(pd.DataFrame(sub_scores, index=[sub]))\n",
    "\n",
    "df_conf = pd.concat(df_conf)\n",
    "df_conf.sort_index(axis=1, inplace=True)\n",
    "df_conf.index.name = 'subj'\n",
    "df_conf.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_conf[['acc_highconf', 'acc_lowconf']].apply(mean_std))\n",
    "pg.ttest(df_conf['acc_highconf'], df_conf['acc_lowconf'], paired=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********\n",
    "\n",
    "## How does YASA perform compared to the other human scorers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus_score(data):\n",
    "    \"\"\"Create consensus score on N-1 scorers (= 4 scorers).\n",
    "    \n",
    "    When ties are present (e.g. [0, 0, 1, 1]), use the scoring of the\n",
    "    most reliable scorer of the record, i.e. the one with the overall strongest\n",
    "    agreement with all the other ones.\n",
    "    \"\"\"\n",
    "    # Reset index so that .loc = .iloc\n",
    "    data = data.reset_index(drop=True)\n",
    "    # Calculate pairwise agreement between scorer\n",
    "    corr_acc = data.replace(STR2NUM).corr(skm.accuracy_score).mean()\n",
    "    # Find index of best scorer\n",
    "    idx_best_scorer = corr_acc.sort_values(ascending=False).index[0]\n",
    "    # Calculate consensus stage\n",
    "    mode, counts = sp_stats.mode(data, axis=1)\n",
    "    mode = np.squeeze(mode)\n",
    "    counts = np.squeeze(counts)\n",
    "    # Find indices of ties\n",
    "    ties = np.where(counts == 2)[0]\n",
    "    # Replace ties values by most reliable scorer of the record\n",
    "    mode[ties] = data.loc[ties, idx_best_scorer].to_numpy()\n",
    "    return mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_scorer = df.columns[df.columns.str.startswith(\"scorer\")].tolist()\n",
    "\n",
    "df_scores_human = []\n",
    "\n",
    "# Loop across nights\n",
    "for sub in tqdm(df['subj'].unique(), leave=False):\n",
    "    df_sub = df[df['subj'] == sub]\n",
    "    for c in cols_scorer:\n",
    "        # Consensus excluding current scorer\n",
    "        other_scorers = np.setdiff1d(cols_scorer, c).tolist()\n",
    "        yt = consensus_score(df_sub[other_scorers])\n",
    "        yp = df_sub[c]\n",
    "        # Calculate performance metrics\n",
    "        sub_scores = {\n",
    "            'subj': sub,\n",
    "            'scorer': c,\n",
    "            'dataset': df_sub['dataset'].iloc[0],\n",
    "            'accuracy': skm.accuracy_score(yt, yp),\n",
    "            'kappa': skm.cohen_kappa_score(yt, yp),\n",
    "            'MCC': skm.matthews_corrcoef(yt, yp),\n",
    "            'f1_macro': skm.f1_score(yt, yp, average='macro', zero_division=1),\n",
    "        }\n",
    "\n",
    "        # F1 for each stage\n",
    "        f1 = skm.f1_score(yt, yp, average=None, labels=labels, zero_division=1)\n",
    "        for f, l in zip(f1, labels):\n",
    "            sub_scores['f1_' + l] = f\n",
    "        \n",
    "        df_scores_human.append(pd.DataFrame(sub_scores, index=[0]))\n",
    "        \n",
    "df_scores_human = pd.concat(df_scores_human, ignore_index=True).set_index([\"subj\", \"scorer\"])\n",
    "# Append YASA scoring\n",
    "df_scores = pd.concat([df_scores, df_scores_human], axis=0, join='inner').sort_index()\n",
    "df_scores.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean + STD\n",
    "df_scores.groupby(\"scorer\").agg(mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranks are calculated separately for each night and then averaged\n",
    "# df_scores.groupby(\"subj\").rank(ascending=False).groupby(\"scorer\").mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise T-test of YASA against the other scorers\n",
    "(df_scores\n",
    " .reset_index()\n",
    " .pairwise_ttests(dv=\"accuracy\", within=\"scorer\", subject=\"subj\", return_desc=True)\n",
    " .drop(columns=['Contrast', 'Paired', \"Parametric\", \"BF10\", \"Tail\", \"hedges\"])\n",
    " .set_index(['A', 'B'])\n",
    " .xs(\"yasa\", level=1, drop_level=False)\n",
    " .round(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for cohen kappa\n",
    "(df_scores\n",
    " .reset_index()\n",
    " .pairwise_ttests(dv=\"kappa\", within=\"scorer\", subject=\"subj\", return_desc=True)\n",
    " .drop(columns=['Contrast', 'Paired', \"Parametric\", \"BF10\", \"Tail\", \"hedges\"])\n",
    " .set_index(['A', 'B'])\n",
    " .xs(\"yasa\", level=1, drop_level=False)\n",
    " .round(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
