{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Dreem-Organization/dreem-learning-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import json\n",
    "import glob\n",
    "import yasa\n",
    "import h5py\n",
    "import pyedflib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from preprocessing import consensus_scores\n",
    "\n",
    "DATASET = 'dodo'\n",
    "\n",
    "# Define paths\n",
    "eeg_dir = \"/Volumes/JAWA/PSG_DATASETS/dod/%s/h5/\" % DATASET\n",
    "hypno_dir = \"/Volumes/JAWA/PSG_DATASETS/dod/%s/scores/\" % DATASET\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "edf_output_dir = \"/Volumes/JAWA/PSG_DATASETS/dod/%s/edf/\" % DATASET\n",
    "\n",
    "if not os.path.isdir(edf_output_dir):\n",
    "    os.mkdir(edf_output_dir)\n",
    "\n",
    "# Scorers and stage mapping\n",
    "scorers = [\"scorer_\" + str(i) for i in [1, 2, 3, 4, 5]]\n",
    "stage_mapping = {'W': 0, \"N1\": 1, \"N2\": 2, \"N3\": 3, \"R\": 4}\n",
    "\n",
    "# YASA classifier\n",
    "model = \"eeg+eog+emg+demo\"\n",
    "path_to_model = parent_dir + '/output/classifiers/clf_%s_lgb_gbdt_custom.joblib' % model\n",
    "scorers = [\"scorer_\" + str(i) for i in [1, 2, 3, 4, 5]]\n",
    "stage_mapping = {'W': 0, \"N1\": 1, \"N2\": 2, \"N3\": 3, \"R\": 4}\n",
    "\n",
    "all_files = sorted(glob.glob(eeg_dir + \"*.h5\"))\n",
    "print(len(all_files), \"files were found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = []\n",
    "\n",
    "for f in tqdm(all_files):\n",
    "    \n",
    "    ###############################################################################################\n",
    "    # LOAD AND PROCESS PSG DATA\n",
    "    ###############################################################################################\n",
    "    \n",
    "    h5 = h5py.File(f, 'r')\n",
    "    fname = h5.attrs['record_id'].decode('UTF-8')\n",
    "    subj = fname.split('-')[0]\n",
    "    # print(fname)\n",
    "    desc = json.loads(h5.attrs['description'])\n",
    "    # list(h5.attrs)\n",
    "    sf = desc[0]['fs']  # = 250 Hz\n",
    "    eeg = h5[\"signals/eeg/C3_M2\"][:]\n",
    "    eog = h5[\"signals/eog/EOG1\"][:]\n",
    "    emg = h5[\"signals/emg/EMG\"][:]\n",
    "    # Check the unit\n",
    "    # display(pd.DataFrame(np.vstack((eeg, eog, emg))).T.describe().round(2))\n",
    "    try:\n",
    "        start_time = datetime.fromtimestamp(h5.attrs['start_time'])\n",
    "    except KeyError:\n",
    "        start_time=None\n",
    "    n_epochs_eeg = eeg.size / sf / 30\n",
    "    \n",
    "    # Export to EDF for validation against other algorithms (apply only once)\n",
    "#     header = pyedflib.highlevel.make_header(patientname=subj, startdate=start_time)\n",
    "#     signal_headers, edf_data = [], []\n",
    "#     for i, c in enumerate(desc):\n",
    "#         # Skip ECG because of invalid physical min/max\n",
    "#         if c['name'] == \"ECG\":\n",
    "#             continue\n",
    "#         # print(c['name'], h5[c['path']][:].min(), h5[c['path']][:].max())\n",
    "#         if c['name'] == \"EOG\":\n",
    "#             c['name'] = c['path'].split('/')[-1]  # EOG1 / EOG2\n",
    "#         signal_headers.append(\n",
    "#             dict(\n",
    "#                 label=c['name'], dimension='uV', sample_rate=c['fs'], physical_min=-2000.0, \n",
    "#                 physical_max=2000.0, digital_min=-32768, digital_max=32767, transducer=\"\", \n",
    "#                 prefilter=\"\"))\n",
    "#         edf_data.append(h5[c['path']][:])\n",
    "\n",
    "#     pyedflib.highlevel.write_edf(\n",
    "#         edf_file=edf_output_dir + subj + \".edf\",\n",
    "#         signals=edf_data, \n",
    "#         signal_headers=signal_headers,\n",
    "#         header=header)\n",
    "    \n",
    "    ###############################################################################################\n",
    "    # LOAD AND PROCESS HYPNOGRAMS\n",
    "    ###############################################################################################\n",
    "    \n",
    "    # Load consensus hypnogram and convert to 2D\n",
    "    hypnos = []\n",
    "    for s in scorers:\n",
    "        hyp_file = hypno_dir + s + \"/\" + fname + \".json\"\n",
    "        if not os.path.isfile(hyp_file): continue\n",
    "        hyp = json.load(open(hyp_file, \"r\"))\n",
    "        hypnos.append(hyp)\n",
    "    if not len(hypnos): continue\n",
    "    hypnos = np.vstack(hypnos)\n",
    "    \n",
    "    # Crop to TIB\n",
    "    if hypnos.shape[1] != n_epochs_eeg:\n",
    "        idx_outside_tib = (hypnos == -1).any(0)\n",
    "        hypnos = hypnos[:, ~idx_outside_tib]\n",
    "\n",
    "    # Check that size matches and pad if needed\n",
    "    if hypnos.shape[1] != n_epochs_eeg:\n",
    "        print(fname, \"HYPNO AND EEG DO NOT MATCH!\", hypnos.shape[1], n_epochs_eeg)\n",
    "        to_pad = int(n_epochs_eeg - hypnos.shape[1])\n",
    "        # if to_pad > 2:\n",
    "            # If the EEG and hypno differ by more than one minute in length, skip subject\n",
    "            # The reason is that we don't know which direction we should pad (before / after)?\n",
    "            # print(\"SKIPPING SUBJECT\")\n",
    "            # continue\n",
    "        # If it's only 1 or 2 epochs, we just repeat the value at the end\n",
    "        hypnos = np.pad(hypnos, [(0, 0), (0, to_pad)], mode=\"edge\")\n",
    "    else:\n",
    "        to_pad = 0\n",
    "        \n",
    "    # Replace -1 by zero in hypnogram\n",
    "    # TODO: Should we just remove these instead?\n",
    "    hypnos[hypnos == -1] = 0\n",
    "    \n",
    "    # Create consensus score (Guillot et al. 2020)\n",
    "    #   To merge multiple sleep stagings into a single consensus sleep staging, we simply take the \n",
    "    #   majority vote on each 30-second epoch. When a tie occurs on a specific epoch, we take the \n",
    "    #   sleep stage scored by the most reliable scorer, i.e. the one with the highest agreement\n",
    "    #   with all the other scorers.\n",
    "    df_hypnos = pd.DataFrame(dict(zip(scorers, hypnos)))\n",
    "    scorer_rank = (\n",
    "        df_hypnos\n",
    "        .corr(accuracy_score)\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "        .index.tolist())\n",
    "    idx_best_hypno = [int(c.split('_')[1]) - 1 for c in scorer_rank][0]\n",
    "    hyp_cons = consensus_scores(hypnos, idx_best_hypno)\n",
    "    \n",
    "    ###############################################################################################\n",
    "    # APPLY YASA\n",
    "    ###############################################################################################\n",
    "    \n",
    "    # Convert PSG data to a Raw array, keeping only a subset of channels\n",
    "    info = mne.create_info(ch_names=['C3_M2', 'EOG1', 'EMG'], \n",
    "                           sfreq=sf, ch_types=['eeg', 'eog', 'emg'])\n",
    "    data = np.vstack((eeg, eog, emg)) / 1e6  # Convert to uV\n",
    "    raw = mne.io.RawArray(data, info, verbose=False)\n",
    "    # Predict sleep stages and confidence\n",
    "    metadata = dict(age=46, male=1)  # Average demographic data from Guillot 2020\n",
    "    sls = yasa.SleepStaging(raw, eeg_name=\"C3_M2\", eog_name=\"EOG1\", emg_name=\"EMG\", metadata=metadata)\n",
    "    hyp_pred = pd.Series(sls.predict(path_to_model)).map(stage_mapping).to_numpy()\n",
    "    assert hyp_pred.size == hyp_cons.size\n",
    "    proba = sls.predict_proba(path_to_model)\n",
    "    confidence = proba.max(1).to_numpy()\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    df_hypnos['cons'] = hyp_cons\n",
    "    df_hypnos['yasa'] = hyp_pred\n",
    "    df_hypnos['confidence'] = confidence\n",
    "    df_hypnos['subj'] = subj\n",
    "    df_hypnos['dataset'] = DATASET\n",
    "    df_hypnos['pad'] = to_pad\n",
    "    df_hypnos.index.name = \"epoch\"\n",
    "    df_pred.append(df_hypnos.reset_index())\n",
    "    \n",
    "df_pred = pd.concat(df_pred, ignore_index=True).set_index([\"dataset\", \"subj\", \"epoch\"])\n",
    "df_pred.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "out_file = parent_dir + \"/output/cv/%s/pred_dreem_%s.csv\" % (model, DATASET)\n",
    "df_pred.to_csv(out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
